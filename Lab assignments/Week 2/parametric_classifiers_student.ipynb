{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Parametric Classifiers\n",
    "Machine Learning 2022/2023 <br>\n",
    "Ruben Wiersma and David Tax\n",
    "\n",
    "Revised by Yorick de Vries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT** This nonmandatory lab consists of several programming and insight exercises/questions on k-nn density estimation.\n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with the basic concepts of parametric classifiers.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a fellow student. If you want to skip right to questions and exercises, find the $\\rightarrow$ symbol.   \n",
    "For questions and feedback please consult the TAs during the lab session. \n",
    "\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayes classifier\n",
    "\n",
    "In this assignment, you will implement your own Bayes classifier. Because this is your first assignment, we will walk you through the steps from loading and inspecting the data, to running your classifier.\n",
    "\n",
    "Specifically, this assignment consists of the following steps:\n",
    "0. Classification using Gaussian distributions\n",
    "1. Getting to know the data\n",
    "2. Validation sets\n",
    "3. Univariate model\n",
    "4. Probability density function\n",
    "5. Posterior probabilities\n",
    "6. Bayes classifier\n",
    "\n",
    "Work your way through these exercises at your own pace and be sure to ask questions to the TA's when you don't understand something. It's important that you get what is happening here, as it is a fundamental building block of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Classification using Gaussian distributions\n",
    "\n",
    "We are starting with a very important notion in machine learning: probability distributions. Occurrences of data typically follow probability distributions that we know how to model.\n",
    "\n",
    "Say, you want to classify apples vs. oranges. A _feature_ that you could use to classify them is their colour. We know, of course, that oranges are orange and apples (the golden delicious kind) are green, but each orange is a slightly different shade of orange. Likewise, the apples are all a different shade of green. If we would plot the colour values against the number of fruits with that colour, we would see, however, that there are probably more oranges with a certain type of shade than with other colours. They tend to follow known probability distributions.\n",
    "\n",
    "In this assignment, we will assume data that has a normal distribution and try to estimate the **parameters** of the assumed normal distribution to correctly fit our data, hence the name **parametric classifiers**. We will then use Bayes' rule to build a classifier based on the probability distribution.\n",
    "\n",
    "Just to refresh your mind, this is what a normal distribution looks like:\n",
    "![Normal distribution for oranges](gaussian.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of apples and oranges, we will try to classify flowers from Fisher's Iris dataset. The dataset contains the measurements of *length* and *width* of the *sepals* and *petals* of 150 flowers. \n",
    "\n",
    "![Petal and sepal in Iris flowers](Petal-sepal.jpg)\n",
    "\n",
    "Using the distribution of these 4 features (*length* and *width* of both *sepals* and *petals*), the flowers can then be classified as one of 3 species of Iris flower:\n",
    "\n",
    "* Iris setosa\n",
    "* Iris versicolor\n",
    "* Iris virginica\n",
    "\n",
    "This dataset is such a classic example that it is even included in machine learning libraries. The following code will load the dataset from `scikit-learn` (this was installed with conda) into the variable `iris`.\n",
    "\n",
    "$\\ex{0.1}$ Run the code and inspect what data is contained in `iris`. Can you identify the 4 attributes? What other information is contained in `iris`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting to know the data\n",
    "\n",
    "The dataset is stored as a dictionary, a data structure in Python that resembles a Java(script) object. We can access items in the dictionary with a dot `.`, so we access the data and their target labels with `iris.data` and `iris.target`, these are both NumPy arrays. If we want to know what each digit means, we can access the names with `iris.target_names`.\n",
    "\n",
    "$\\ex{1.1}$ Run the code fragment and confirm what it is doing. Try to understand the indexing and print the following data:\n",
    "- The last five flowers. Expected result: an array with shape (5, 4).\n",
    "- Only the third feature of each flower. Expected result: an array with shape (150,).\n",
    "- The names of the first ten flowers. Expected result: an array with shape (10,).\n",
    "- Three separate arrays (one for each class). Expected result: three arrays with shape (50, 4). Try doing this without assuming anything about the indices for each class, i.e.: do not simply use `class1 = iris.data[:50, :]`. You can use `np.where`. This function takes in a boolean statement and returns the indices for which the statement is true. Example use: `np.where(iris.target == 0)` returns all indices where the target label is 0.\n",
    "\n",
    "__Hint__ Look at the indexing chapter in last week's NumPy lab for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five flowers: \n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "Their labels:  [0 0 0 0 0]\n",
      "And the label names:  ['setosa' 'versicolor' 'virginica']\n",
      "Last five flowers: \n",
      " None\n",
      "Only the third feature:  None\n",
      "All label names:  None\n",
      "Class:  setosa ; Items: \n",
      " None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll label names: \u001b[39m\u001b[38;5;124m\"\u001b[39m, first_ten_names)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass: \u001b[39m\u001b[38;5;124m\"\u001b[39m, iris\u001b[38;5;241m.\u001b[39mtarget_names[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; Items: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, setosa_flowers)\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mlast_five_flowers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m4\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a two dimensional array of shape (5,4)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m third_feature_only\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m150\u001b[39m,), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected an array of shape (150,)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m first_ten_names\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m10\u001b[39m,), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected an array of shape (10,)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(\"First five flowers: \\n\", iris.data[:5, :])\n",
    "print(\"Their labels: \", iris.target[:5])\n",
    "print(\"And the label names: \", iris.target_names)\n",
    "\n",
    "last_five_flowers = None\n",
    "third_feature_only = None\n",
    "first_ten_names = None\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "setosa_flowers = None\n",
    "versicolor_flowers = None\n",
    "virginica_flowers = None\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "\n",
    "print(\"Last five flowers: \\n\", last_five_flowers)\n",
    "print(\"Only the third feature: \", third_feature_only)\n",
    "print(\"All label names: \", first_ten_names)\n",
    " \n",
    "print(\"Class: \", iris.target_names[0], \"; Items: \\n\", setosa_flowers)\n",
    "\n",
    "assert last_five_flowers.shape == (5,4), \"Expected a two dimensional array of shape (5,4)\"\n",
    "assert third_feature_only.shape == (150,), \"Expected an array of shape (150,)\"\n",
    "assert first_ten_names.shape == (10,), \"Expected an array of shape (10,)\"\n",
    "\n",
    "assert setosa_flowers.shape == (50,4), \"Expected a two dimensional array of shape (50,4)\"\n",
    "assert versicolor_flowers.shape == (50,4), \"Expected a two dimensional array of shape (50,4)\"\n",
    "assert virginica_flowers.shape == (50,4), \"Expected a two dimensional array of shape (50,4)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to get an idea of the distribution of our data, we can make plots.\n",
    "\n",
    "$\\ex{1.2}$ Run the following code to plot the petal length and width of each flower as a scatterplot. Inspect the code carefully, as you will need to write your own code for plotting later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From the Matplotlib library, import pyplot. We will refer to this library later as plt.\n",
    "# This is a widely used library that lets you create images and plot your data.\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create a scatterplot of the first two features, and use their labels as colour values.\n",
    "plt.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.show()\n",
    "# Create a scatterplot of the third and fourth feature.\n",
    "plt.scatter(iris.data[:, 2], iris.data[:, 3], c=iris.target)\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{1.1}$ How are the points distributed? Could you fit a probability distribution that you know on this data (e.g. uniform, normal, etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test sets\n",
    "\n",
    "Now that we have an idea what our dataset looks like, our goal is to create a model that will predict the class of each flower based on its features. In order to evaluate how well the model fits, we will also need a separate test set where we can evaluate our final model on. For this, we will split the data randomly in a train and test set.\n",
    "\n",
    "$\\ex{2.1}$ Use the code below to split the dataset into a train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #to split in train and test set\n",
    "\n",
    "# load the data and create the training and test sets\n",
    "iris = datasets.load_iris()\n",
    "# X is the feature vectors for the data points, and Y is the target (ground truth) class for those data points \n",
    "# the iris.data and iris.target entries are randomly divided into training and test sets.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=20)\n",
    "\n",
    "# Due to the randomness of the split, number of each flowers is not necessarily the same\n",
    "# Separate the training dataset into the three flower types.\n",
    "setosa_X_train = None\n",
    "versicolor_X_train = None\n",
    "virginica_X_train = None\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "assert setosa_X_train.shape[0] != versicolor_X_train.shape[0]\n",
    "assert setosa_X_train.shape[0] != virginica_X_train.shape[0]\n",
    "assert versicolor_X_train.shape[0] != virginica_X_train.shape[0]\n",
    "\n",
    "setosa_X_train.shape, versicolor_X_train.shape, virginica_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Univariate model\n",
    "\n",
    "Looking at the plots of the data from the previous section, you might assume that separating the different classes would be a lot easier based on the petal data (3rd and 4th variable) than on the sepal data (1st and 2nd variable), as it is easier to distinguish the different clusters in that plot. In fact, for now we will only focus on one variable, the petal length (3rd feature), as it looks like it might be useful just on its own and this will simplify the model a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use the third feature\n",
    "feature_idx = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at the distribution of all flowers (both train and test) along this feature to confirm that our assumption of a normal distribution is correct. Take a look at the ditribution of the other features as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(setosa_flowers[:,feature_idx], label=iris.target_names[0])\n",
    "plt.hist(versicolor_flowers[:,feature_idx], label=iris.target_names[1])\n",
    "plt.hist(virginica_flowers[:,feature_idx], label=iris.target_names[2])\n",
    "plt.xlabel(iris.feature_names[feature_idx])\n",
    "plt.ylabel('Number of flowers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks about correct! Now, let's find the parameters of the normal distribution that describe our data best. The parameters that we need to describe the distribution are the _mean_ and _standard deviation_.\n",
    "\n",
    "$\\ex{3.1}$ Using the training data from each of 3 classes, compute the mean ($\\mu$) and standard deviation ($\\sigma$) for the *petal length* attribute. The Maximum Likelihood Estimators for these are given by\n",
    "\n",
    "(3.1) $$\\mu = \\frac{\\sum_{t=1}^Nx^t}{N}$$\n",
    "\n",
    "(3.2) $$\\sigma = \\sqrt{\\frac{\\sum_{t=1}^N(x^t - m)^2}{N}}$$\n",
    "\n",
    "__Hint__ Try to use numpy's functions to perform operations on your input (e.g. `np.sum`, `np.sqrt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mean(x):\n",
    "    mean = 0\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return mean\n",
    "    \n",
    "def compute_sd(x, mean):\n",
    "    sd = 0\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return sd\n",
    "\n",
    "# Compute the mean for each flower type.\n",
    "mean_setosa = compute_mean(setosa_X_train[:, feature_idx])\n",
    "mean_versicolor = compute_mean(versicolor_X_train[:, feature_idx])\n",
    "mean_virginica = compute_mean(virginica_X_train[:, feature_idx])\n",
    "\n",
    "# Compute the standard deviation for each flower type.\n",
    "sd_setosa = compute_sd(setosa_X_train[:, feature_idx], mean_setosa)\n",
    "sd_versicolor = compute_sd(versicolor_X_train[:, feature_idx], mean_versicolor)\n",
    "sd_virginica = compute_sd(virginica_X_train[:, feature_idx], mean_virginica)\n",
    "\n",
    "# Print the computed means and standard deviations.\n",
    "print(\"setosa\", mean_setosa, sd_setosa)\n",
    "print(\"versicolor\", mean_versicolor, sd_versicolor)\n",
    "print(\"virginica\", mean_virginica, sd_virginica)\n",
    "\n",
    "assert np.isclose(mean_setosa, 1.4729729729729728), \"Expected a different mean\"\n",
    "assert np.isclose(mean_versicolor, 4.25), \"Expected a different mean\"\n",
    "assert np.isclose(mean_virginica, 5.572222222222222), \"Expected a different mean\"\n",
    "\n",
    "assert np.isclose(sd_setosa, 0.17652600857089654), \"Expected a different standard deviation\"\n",
    "assert np.isclose(sd_versicolor, 0.44300112866673375), \"Expected a different standard deviation\"\n",
    "assert np.isclose(sd_virginica, 0.547017728288333), \"Expected a different standard deviation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{3.1}$ Do these mean values and standard deviations correspond to the histograms that we plotted? If not, try to fix your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Probability density function\n",
    "\n",
    "The probability density function for a Gaussian distribution is defined as\n",
    "\n",
    "(4.1) $$p(x|\\mu, \\sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "That means that if we have estimates for $\\mu$ and $\\sigma$, we can compute the probability density for a specific value $x$.\n",
    "\n",
    "$\\ex{4.1}$ Implement the `normal_PDF` function below. Given `x`, `mean`, and `sd`, we want to return the result of $p(x|\\mu, \\sigma)$. Your PDF is plotted. You can play around with different configurations of `mean` and `sd` to see how these parameters influence your normal distribution.\n",
    "\n",
    "**NOTE:** This normal distribution should look similar to the normal distribution at chapter 0. (This normal distribution does have a different mean and standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def normal_PDF(x, mean, sd):\n",
    "    pdf = 0\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return pdf\n",
    "\n",
    "# Set x, mean and standard deviation\n",
    "x = 0.5\n",
    "mean = 2\n",
    "sd = 0.5\n",
    "my_pdf = normal_PDF(x, mean, sd)\n",
    "\n",
    "# You can compare your outcome to scipy's built-in normal PDF\n",
    "scipy_pdf = norm.pdf(x, mean, sd)\n",
    "print(\"Your pdf function outcome: \", my_pdf, \" Scipy's function outcome: \", scipy_pdf)\n",
    "assert np.isclose(my_pdf, scipy_pdf)\n",
    "\n",
    "# And we plot the result of your PDF function for 100 points between 0 and 4: np.linspace(0, 4, 100)\n",
    "xs = np.linspace(0, 4, 100)\n",
    "plt.plot(xs, normal_PDF(xs, mean, sd))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already made estimates for $\\mu$ and $\\sigma$ for the *petal length* for each of the 3 classes, so we can now also define PDFs for each separate class.\n",
    "\n",
    "$\\ex{4.2}$ Plot the 3 functions using [linspace](https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.linspace.html) for a range of x-values aside the histograms of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Histograms of the flower types of the training set\n",
    "plt.hist(setosa_X_train[:,feature_idx], label=iris.target_names[0])\n",
    "plt.hist(versicolor_X_train[:,feature_idx], label=iris.target_names[1])\n",
    "plt.hist(virginica_X_train[:,feature_idx], label=iris.target_names[2])\n",
    "\n",
    "# Plot your PDFs here\n",
    "xs = np.linspace(0, 7, 100)\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "plt.xlabel(iris.feature_names[feature_idx])\n",
    "plt.ylabel('Number of flowers / PDF')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{4.1}$ Do your distributions and the histogram overlap? In what ways are the histogram and the probability distributions different?\n",
    "\n",
    "__Hint__ The histogram shows the number of flowers that have a petal length within a certain window (bin). That means the values shown in the histogram are absolute counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Posterior probabilities\n",
    "\n",
    "The plot above shows the probability densities for a feature $x$. For a normal distributed feature of a class $C_i$, you only need to know the mean and the standard deviation to be able to determine the probability of obtaining that data point $x$, i.e. $p(x | \\mu_i, \\sigma_i)$ or $p(x | C_i)$. So, $p(x | C_i)$ is the probability density of observing $x$ knowing that the datapoint comes from $C_i$.\n",
    "\n",
    "- The prior probabillity is the probability density of a certain class $C_i$, without having any observations (knowledge); $p(C_i)$.\n",
    "- The posterior probabillity is the probability density of a certain class $C_i$, knowing a datapoint $x$ you observed; $p(C_i | x)$.\n",
    "\n",
    "$\\q{5.1}$ Stop for a moment to try and understand what this probability means: $p(x | C_i)$.\n",
    "\n",
    "__Hint__ The $|$ sign in $p(x | C_i)$ means: given that.\n",
    "\n",
    "\n",
    "However, what would be useful for classification, is the posterior probabilities of the classes given the data, i.e. $P(C_i | x)$.\n",
    "\n",
    "$\\q{5.2}$ Can you explain this mathematical formulation in your own words?\n",
    "\n",
    "$\\q{5.3}$ Why is it helpful to know the posterior probability?\n",
    "\n",
    "__Hint__ What information do we have for test points coming in?\n",
    "\n",
    "\n",
    "To get the posterior probability, we can use Bayes' rule:\n",
    "\n",
    "(5.1) $$P(C_i | x) =  \\frac{p(x | C_i) P(C_i)}{p(x)} = \\frac{p(x | C_i) P(C_i)}{\\sum_{k=1}^K p(x | C_k) P(C_k)}$$\n",
    "\n",
    "We will construct our classifier such that, after observing a datapoint $x$, we assign the point to the class $C_i$ with the higest $P(C_i | x)$, so to the class which is most likely.\n",
    "\n",
    "$\\ex{5.1}$ Finish the code to compute the posterior probability of a point $x$, given the mean, standard deviation, and class index.\n",
    "\n",
    "__Hint__ The mean and standard deviation are given as arrays. You can access the mean for class `i` with `mean[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def posterior(x, means, sds, priors, i):\n",
    "    \"\"\"\n",
    "    Compute the posterior probability P(C_i | x).\n",
    "    :param x: the sample to compute the posterior probability for.\n",
    "    :param means: an array of means for each class.\n",
    "    :param sds: an array of standard deviation values for each class.\n",
    "    :param priors: an array of frequencies for each class.\n",
    "    :param i: the index of the class to compute the posterior probability for.\n",
    "    \"\"\"\n",
    "    posterior = 0\n",
    "    # START ANSWER\n",
    "    #END ANSWER\n",
    "    return posterior\n",
    "\n",
    "means = [mean_setosa, mean_versicolor, mean_virginica]\n",
    "sds = [sd_setosa, sd_versicolor, sd_virginica]\n",
    "priors = [\n",
    "    setosa_X_train.shape[0]/X_train.shape[0],\n",
    "    versicolor_X_train.shape[0]/X_train.shape[0],\n",
    "    virginica_X_train.shape[0]/X_train.shape[0]\n",
    "]\n",
    "\n",
    "# Test out the code\n",
    "flower_idx = 6\n",
    "print(\"Flower belongs to class\", iris.target_names[Y_train[flower_idx]])\n",
    "\n",
    "# iterate over all classes\n",
    "for i in range(3):\n",
    "    x_post = posterior(X_train[flower_idx, feature_idx], means, sds, priors, i)\n",
    "    print(\"Posterior probability for class\", iris.target_names[i], \": \", x_post)\n",
    "\n",
    "post_setosa = posterior(X_train[flower_idx, feature_idx], means, sds, priors, 0)\n",
    "post_versicolor = posterior(X_train[flower_idx, feature_idx], means, sds, priors, 1)\n",
    "post_virginica = posterior(X_train[flower_idx, feature_idx], means, sds, priors, 2)\n",
    "\n",
    "assert np.isclose(post_setosa, 1.1048294835009998e-107, rtol = 0.0001, atol = 0.), \"Expected a different posterior probability\"\n",
    "assert np.isclose(post_versicolor, 0.03817178391547811, rtol = 0.0001, atol = 0.), \"Expected a different posterior probability\"\n",
    "assert np.isclose(post_virginica, 0.9618282160845218, rtol = 0.0001, atol = 0.), \"Expected a different posterior probability\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{5.2}$ Plot the posterior probabilities for all 3 classes. Does the plot of these 3 posteriors make sense based on the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(0, 7, 100)\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "plt.xlabel(iris.feature_names[feature_idx])\n",
    "plt.ylabel('Posterior probability')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{5.4}$ Where would you put the decision boundary for each class? In other words: where would you draw the line, separating each class. Could you formulate this mathematically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bayes Classifier\n",
    "\n",
    "Now that we can compute the posteriors for every class, constructing a classifier is easy. The Bayes classifier is defined as\n",
    "\n",
    "- Classify as $C_i$ for which: $i = argmax_i\\ P(C_i |x)$\n",
    "\n",
    "$\\ex{6.1}$ Write the code for the `classify` function. It should classify a single data point $x$ as one of the 3 classes, returning $0$, $1$ or $2$ based on the class the flower is most likely to belong to. The other arguments of the function should therefore be the vector of mean estimates `means` and the vector of standard deviation estimates `sds`, the class distribution `priors` and index `i` corresponds to class $C_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(x, means, sds, priors):\n",
    "    classification = -1\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return classification\n",
    "\n",
    "# Test out the code\n",
    "flower_idxs = [5,20,30]\n",
    "predicted_classes = np.zeros(3, dtype=np.int64)\n",
    "for i, flower_idx in enumerate(flower_idxs):\n",
    "    predicted_classes[i] = classify(X_train[flower_idx, feature_idx], means, sds, priors)\n",
    "\n",
    "print(\"Predicted class\", iris.target_names[predicted_classes])\n",
    "print(\"Flower belongs to class\", iris.target_names[Y_train[flower_idxs]])\n",
    "assert (predicted_classes == Y_train[flower_idxs]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{6.2}$ Finally, complete the `evaluate` function below. It should take a test set, the expected classes (ground truth; correct classifications of each element in the test set) and the vectors `means`, `sds` and `priors`. The function calculates the classifications (decides which class each point belongs to) based on the distributions learnt from the training set. The function should return the percentage of elements in the test set that were classified correctly.\n",
    "\n",
    "**Note** We only use the test set now to be able to make an unbiased estimation of the quality of the classifier. If we would include the testset in our training data, the evaluation would not be fair as knowledge of the testdata would be included in the model.\n",
    "\n",
    "__Hint__ You will only need to use the *petal length* variable from each data point to attempt to classify it (since that is how we trained our model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(X_test, Y_test, means, sds, priors):\n",
    "    accuracy = 0\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate(X_test[:, feature_idx], Y_test, means, sds, priors)\n",
    "\n",
    "print(accuracy)\n",
    "assert accuracy > 0.9, \"Expected a higher accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to our scatterplots and see how your classifier makes decisions. For this, we also plot the decision boundary. The function to create the decision boundaries does this in a very simple way:\n",
    "- For each class, compute the posterior for 1000 points between 1 and 7\n",
    "- If for any two classes the posteriors are as good as equal (and not very close to 0) at a point, add that point to the list of decision boundaries\n",
    "- Plot vertical lines at these points\n",
    "\n",
    "$\\q{6.1}$ This method is quite complicated. Could you analytically solve the equations?\n",
    "\n",
    "__Hint__ You have to find the $x$ for which the class probabilities are equal. You can formulate this equality with the probability functions and solve that equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decision_boundary(means, sds, priors):\n",
    "    decision_boundaries = []\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return decision_boundaries\n",
    "\n",
    "# Create a scatterplot of the third and fourth feature.\n",
    "feature_idx2 = 3\n",
    "\n",
    "plt.scatter(iris.data[:, feature_idx], iris.data[:, feature_idx2], c=iris.target)\n",
    "plt.xlabel(iris.feature_names[feature_idx])\n",
    "plt.ylabel(iris.feature_names[feature_idx2])\n",
    "decision_boundaries = decision_boundary(means, sds, priors)\n",
    "for boundary in decision_boundaries:\n",
    "    plt.axvline(x=boundary)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{6.2}$ You have now successfully implemented a Bayes-classifier. Try to change the feature used in the classifier `feature_idx` and see how it affects the performance. Was the choice of petal width a good choice? And was there another feature which performs good as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ef525aed409cc5a13783047c8d4d090b051bbc20387ad523d849e44a70e3fe1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
