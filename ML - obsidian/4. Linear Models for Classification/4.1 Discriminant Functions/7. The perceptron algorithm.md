# The perceptron algorithm
Another example of a linear discriminant model is the perceptron of Rosenblatt
(1962), which occupies an important place in the history of pattern recognition algorithms. It corresponds to a two-class model in which the input vector *x* is first
transformed using a fixed nonlinear transformation to give a feature vector $\phi(x)$,
and this is then used to construct a generalized linear model of the form
$$
y(x) = f\ (w^T\phi(x))
\tag{4.52}
$$
where the nonlinear activation function $f(·)$ is given by a step function of the form
$$
\begin{equation}
  f(a)=\begin{cases}
    +1, & a \geq 0 \\
    -1, & a \lt 0
  \end{cases}
\end{equation}
\tag{4.53}
$$
The vector $\phi(x)$ will typically include a bias component $\phi_0(x) = 1$. In earlier
discussions of two-class classification problems, we have focussed on a target coding scheme in which $t \in \{0, 1\}$, which is appropriate in the context of probabilistic models. For the perceptron, however, it is more convenient to use target values$ t = +1$ for class $C_1$ and $t = −1$ for class $C_2$, which matches the choice of activation function.

The algorithm used to determine the parameters **w** of the perceptron can most
easily be motivated by error function minimization. A natural choice of error function would be the total number of misclassified patterns. However, this does not lead to a simple learning algorithm because the error is a piecewise constant function of **w**, with discontinuities wherever a change in **w** causes the decision boundary to move across one of the data points. Methods based on changing **w** using the gradient of the error function cannot then be applied, because the gradient is zero almost everywhere.

We therefore consider an alternative error function known as the [[perceptron criterion]]. To derive this, we note that we are seeking a weight vector **w** such that
patterns $x_n$ in class $C_1$ will have $w^T\phi(xn) \gt 0$, whereas patterns $x_n$ in class $C_2$ have $w^T\phi(x_n) \lt 0$. Using the $t \in \{−1, +1\}$ target coding scheme it follows that we would like all patterns to satisfy $w^T\phi(x_n)t_n \gt 0$. The perceptron criterion associates zero error with any pattern that is correctly classified, whereas for a misclassified pattern $x_n$ it tries to minimize the quantity $−w^T\phi(x_n)t_n$. The perceptron criterion is therefore given by
$$
E_\rho(w) = -\sum_{n \in M}w^T\phi_nt_n
\tag{4.54}
$$
where *M* denotes the set of all misclassified patterns. The contribution to the error
associated with a particular misclassified pattern is a linear function of w in regions
of w space where the pattern is misclassified and zero in regions where it is correctly classified. The total error function is therefore piecewise linear.

We now apply the stochastic gradient descent algorithm to this error function.
The change in the weight vector **w** is then given by
$$
w^{(\tau+1)} = w^{(\tau)} - \eta \nabla E_p(w) = w^{(\tau)}+\eta\phi_nt_n
\tag{4.55}
$$
where $\eta$ is the learning rate parameter and τ is an integer that indexes the steps of
the algorithm. Because the perceptron function $y(x,w)$ is unchanged if we multiply
**w** by a constant, we can set the learning rate parameter $\eta$ equal to 1 without of
generality. Note that, as the weight vector evolves during training, the set of patterns that are misclassified will change.

The perceptron learning algorithm has a simple interpretation, as follows. We
cycle through the training patterns in turn, and for each pattern $x_n$ we evaluate the
perceptron function (4.52). If the pattern is correctly classified, then the weight
vector remains unchanged, whereas if it is incorrectly classified, then for class $C_1$
we add the vector $\phi(x_n)$ onto the current estimate of weight vector w while for
class $C_2$ we subtract the vector$\phi(x_n)$ from **w**. The perceptron learning algorithm is
illustrated in [[Figure 4.7.png|Figure 4.7]].

![[Figure 4.7.png]]
[[Figure 4.7.png|Figure 4.7]]

If we consider the effect of a single update in the perceptron learning algorithm,
we see that the contribution to the error from a misclassified pattern will be reduced because from (4.55) we have
$$
-w^{(\tau+1)T}\phi_nt_n = -w^{(\tau)T}\phi_nt_n - (\phi_nt_n)^T\phi_nt_n \lt -w^{(\tau)T}\phi_nt_n
\tag{4.56}
$$
where we have set $\eta$ = 1, and made use of $||\phi_nt_n||^2 \gt 0$. Of course, this does
not imply that the contribution to the error function from the other misclassified
patterns will have been reduced. Furthermore, the change in weight vector may have caused some previously correctly classified patterns to become misclassified. Thus the perceptron learning rule is not guaranteed to reduce the total error function at each stage.

However, the [[perceptron convergence theorem]] states that if there exists an exact
solution (in other words, if the training data set is linearly separable), then the
perceptron learning algorithm is guaranteed to find an exact solution in a finite number of steps. Proofs of this theorem can be found for example in Rosenblatt (1962), Block (1962), Nilsson (1965), Minsky and Papert (1969), Hertz et al. (1991), and Bishop (1995a). Note, however, that the number of steps required to achieve convergence could still be substantial, and in practice, until convergence is achieved, we will not be able to distinguish between a nonseparable problem and one that is simply slow to converge.

Even when the data set is linearly separable, there may be many solutions, and
which one is found will depend on the initialization of the parameters and on the order of presentation of the data points. Furthermore, for data sets that are not linearly separable, the perceptron learning algorithm will never converge.

Aside from difficulties with the learning algorithm, the perceptron does not provide
probabilistic outputs, nor does it generalize readily to $K \gt 2$ classes. The most
important limitation, however, arises from the fact that (in common with all of the
models discussed in this chapter and the previous one) it is based on linear combinations of fixed basis functions. More detailed discussions of the limitations of perceptrons can be found in Minsky and Papert (1969) and Bishop (1995a).

Analogue hardware implementations of the perceptron were built by Rosenblatt,
based on motor-driven variable resistors to implement the adaptive parameters wj .
These are illustrated in [[Figure 4.8.png|Figure 4.8]]. The inputs were obtained from a simple camera
system based on an array of photo-sensors, while the basis functions φ could be
chosen in a variety of ways, for example based on simple fixed functions of randomly chosen subsets of pixels from the input image. Typical applications involved learning to discriminate simple shapes or characters.

![[Figure 4.8.png]]
[[Figure 4.8.png|Figure 4.8]]

At the same time that the perceptron was being developed, a closely related
system called the [[adaline]], which is short for ‘adaptive linear element’, was being
explored byWidrow and co-workers. The functional form of the model was the same as for the perceptron, but a different approach to training was adopted (Widrow and Hoff, 1960; Widrow and Lehr, 1990).