Now consider the extension of linear discriminants to $K \gt 2$ classes. We might be tempted be to build a *K*-class discriminant by combining a number of two-class discriminant functions. However, this leads to some serious difficulties (Duda and Hart, 1973) as we now show.

Consider the use of *K*−1 classifiers each of which solves a two-class problem of separating points in a particular class $C_k$ from points not in that class. This is known as a [[one-versus-the-rest]] classifier. The left-hand example in [[Figure 4.2.png|Figure 4.2]] shows an example involving three classes where this approach leads to regions of input space that are ambiguously classified.

![[Figure 4.2.png]]
[[Figure 4.2.png|Figure 4.2]]

An alternative is to introduce $K(K − 1)/2$ binary discriminant functions, one for every possible pair of classes. This is known as a [[one-versus-one classifier]]. Each point is then classified according to a majority vote amongst the discriminant functions. However, this too runs into the problem of ambiguous regions, as illustrated in the right-hand diagram of [[Figure 4.2.png|Figure 4.2]].

We can avoid these difficulties by considering a single *K*-class discriminant comprising *K* linear functions of the form
$$
y_k(x) = w^T_kx + w_{k0}
\tag{4.9}
$$
and then assigning a point x to class $C_k$ if $y_k(x) \gt y_j(x)$ for all $j \neq k$. The decision boundary between class $C_k$ and class $C_j$ is therefore given by $y_k(x) = y_j(x)$ and hence corresponds to a (*D* − 1)-dimensional hyperplane defined by
$$
(w_k - w_j)^Tx + (w_{k0}-w_{j0}) = 0
\tag{4.10}
$$
This has the same form as the decision boundary for the two-class case discussed in [[Section 4.1.1]], and so analogous geometrical properties apply.

The decision regions of such a discriminant are always singly connected and convex. To see this, consider two points $x_A$ and $x_B$ both of which lie inside decision region $R_k$, as illustrated in Figure 4.3. Any point $\widehat{x}$ that lies on the line connecting $x_A$ and $x_B$ can be expressed in the form
$$
\widehat{x} = \lambda x_A+(1-\lambda)x_B
\tag{4.11}
$$
where $0 \leq \lambda \leq 1$. From the linearity of the discriminant functions, it follows that
$$
y_k(\widehat{x}) = \lambda y_k(x_A) + (1-\lambda)y_k(x_b)
\tag{4.12}
$$
Because both $x_A$ and $x_B$ lie inside $R_k$, it follows that $y_k(x_A) \gt y_j(x_A)$, and $y_k(x_B) \gt y_j(x_B)$, for all $j \neq k$, and hence $y_k(\widehat{x}) \gt yj(\widehat{x})$, and so $\widehat{x}$ also lies inside Rk. Thus $R_k$ is singly connected and convex.

Note that for two classes, we can either employ the formalism discussed here, based on two discriminant functions $y_1(x)$ and $y_2(x)$, or else use the simpler but equivalent formulation described in [[Section 4.1.1]] based on a single discriminant function y(x).

We now explore three approaches to learning the parameters of linear discriminant functions, based on least squares, Fisher’s linear discriminant, and the perceptron algorithm.
