# Multiclass logistic regression
In our discussion of generative models for multiclass classification, we have seen that for a large class of distributions, the posterior probabilities are given by a softmax transformation of linear functions of the feature variables, so that
$$
p(C_k|\phi) = y_k(\phi) = \frac{exp(a_k)}{\sum_jexp(a_j)}
\tag{4.104}
$$
where the ‘activations’ $a_k$ are given by
$$
a_k = w^T_k\phi
\tag{4.105}
$$
There we used maximum likelihood to determine separately the class-conditional
densities and the class priors and then found the corresponding posterior probabilities using Bayes’ theorem, thereby implicitly determining the parameters $\{w_k\}$. Here we consider the use of maximum likelihood to determine the parameters $\{w_k\}$ of this model directly. To do this, we will require the derivatives of $y_k$ with respect to all of the activations $a_j$ . These are given by
$$
\frac{\partial y_k}{\partial a_j}y_k(I_{kj}-y_j)
\tag{4.106}
$$
where $I_{kj}$ are the elements of the identity matrix.

Next we write down the likelihood function. This is most easily done using the 1-of-*K* coding scheme in which the target vector $t_n$ for a feature vector $\phi_n$ belonging to class $C_k$ is a binary vector with all elements zero except for element *k*, which equals one. The likelihood function is then given by
$$
p(T|w_1,...,w_k)=\prod^N_{n=1}\prod^K_{k=1}p(C_k|\phi_n)^{t_{nk}} = \prod^N_{n=1}\prod^K_{k=1}y^{t_{nk}}_{nk}
\tag{4.107}
$$
where $y_{nk} = y_k(\phi_n)$, and **T** is an $N × K$ matrix of target variables with elements
$t_{nk}$. Taking the negative logarithm then gives
$$
E(w_1,...,w_k) = -ln\ p(T|w_1,...,w_k) = -\sum^N_{n=1}\sum^K_{k=1}t_{nk}ln\ y_{nk}
\tag{4.108}
$$
which is known as the [[cross-entropy]] error function for the multiclass classification
problem.

We now take the gradient of the error function with respect to one of the parameter vectors wj . Making use of the result (4.106) for the derivatives of the softmax function, we obtain
$$
\nabla w_jE(w_1,...,w_k) = \sum^N_{n=1}(y_{nj}-t_{nj})\phi_n
\tag{4.109}
$$
where we have made use of $\sum_k t_{nk} = 1$. Once again, we see the same form arising
for the gradient as was found for the sum-of-squares error function with the linear
model and the cross-entropy error for the logistic regression model, namely the product of the error $(y_{nj} − t_{nj})$ times the basis function $\phi_n$. Again, we could use this
to formulate a sequential algorithm in which patterns are presented one at a time, in which each of the weight vectors is updated using (3.22).

We have seen that the derivative of the log likelihood function for a linear regression model with respect to the parameter vector *w* for a data point *n* took the form of the ‘error’ $y_n − t_n$ times the feature vector $\phi_n$. Similarly, for the combination of logistic sigmoid activation function and cross-entropy error function (4.90), and for the softmax activation function with the multiclass cross-entropy error function (4.108), we again obtain this same simple form. This is an example of a more general result, as we shall see in [[Section 4.3.6]].

To find a batch algorithm, we again appeal to the Newton-Raphson update to
obtain the corresponding IRLS algorithm for the multiclass problem. This requires
evaluation of the Hessian matrix that comprises blocks of size $M × M$ in which
block *j*, *k* is given by
$$
\nabla_{wk}\nabla_{wjE(w_1,...,w_k)} = -\sum^N_{n=1}y_{nk}(I_{kj} - y_{nj})\phi_n\phi_n^T
\tag{4.110}
$$
As with the two-class problem, the Hessian matrix for the multiclass logistic regression model is positive definite and so the error function again has a unique minimum. Practical details of IRLS for the multiclass case can be found in Bishop and Nabney (2008).