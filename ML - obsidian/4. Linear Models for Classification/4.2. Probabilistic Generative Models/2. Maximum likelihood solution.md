# maximum likelihood solution
Once we have specified a parametric functional form for the class-conditional densities p(x|Ck), we can then determine the values of the parameters, together with the prior class probabilities p(Ck), using maximum likelihood. This requires a data set comprising observations of x along with their corresponding class labels.

Consider first the case of two classes, each having a Gaussian class-conditional density with a shared covariance matrix, and suppose we have a data set {xn, tn} where n = 1, . . . , N. Here tn = 1 denotes class C1 and tn = 0 denotes class C2. We denote the prior class probability p(C1) = π, so that p(C2) = 1−π. For a data point xn from class C1, we have tn = 1 and hence
$$
p(x_n,C_1) = p(C_1)p(x_n|C_1) = \pi N(x_n|\mu_1,\Sigma)
$$
Similarly for class $C_2$, we have tn = 0 and hence
$$
p(x_n,C_2) = p(C_2)p(x_n|C_2) = (1 - \pi)N(x_n|\mu_2,\Sigma)
$$
Thus the likelihood function is given by
$$
p(t|\pi,\mu_1,\mu_2,\Sigma) = \prod^N_{n=1}[\pi N(x_n|/mu_1,\Sigma)]^{t_n}[(1 - \pi)N(x_n|\mu_2,\Sigma)]^{1-t_n}
\tag{4.71}
$$
where $t = (t_1, ..., t_N)^T$. As usual, it is convenient to maximize the log of the likelihood function. Consider first the maximization with respect to $\pi$. The terms in the log likelihood function that depend on $\pi$ are
$$
\sum^N_{n=1}\{t_nln(\pi + (1 - t_N))ln(1 - \pi)\}
\tag{4.72}
$$
Setting the derivative with respect to π equal to zero and rearranging, we obtain
$$
\pi = \frac{1}{N}\sum^N_{n=1}t_n = \frac{N_1}{N} = \frac{N_1}{N_1+N_2}
\tag{4.73}
$$
whereN1 denotes the total number of data points in class C1, andN2 denotes the total number of data points in class C2. Thus the maximum likelihood estimate for π is simply the fraction of points in class C1 as expected. This result is easily generalized to the multiclass case where again the maximum likelihood estimate of the prior probability associated with class Ck is given by the fraction of the training set points assigned to that class.

Now consider the maximization with respect to μ1. Again we can pick out of the log likelihood function those terms that depend on μ1 giving
$$
\sum^N_{n=1}t_nln(N(x_n|\mu_1,\Sigma)) = -\frac{1}{2}\sum^N_{n=1}t_n(x_n-\mu_1)^T\Sigma^{-1}(x_n-\mu_1) + const
\tag{4.74}
$$
Setting the derivative with respect to $\mu_1$ to zero and rearranging, we obtain
$$
\mu_1 = \frac{1}{N_1}\sum^N_{n=1}t_nx_n
\tag{4.75}
$$
which is simply the mean of all the input vectors $x_n$ assigned to class $C_1$. By a similar argument, the corresponding result for $\mu_2$ is given by
$$
\mu_2 = \frac{1}{N_2}\sum^N_{n=1}(1-t_n)x_n
\tag{4.76}
$$
which again is the mean of all the input vectors $x_n$ assigned to class $C_2$.

Finally, consider the maximum likelihood solution for the shared covariance matrix $\Sigma$. Picking out the terms in the log likelihood function that depend on $\Sigma$, we have
$$
\begin{align}
-\frac{1}{2}\sum^N_{n=1}t_nln|\Sigma|- -\frac{1}{2}\sum^N_{n=1}t_n(x_n-\mu_1)^T\Sigma^{-1}(x_n-\mu_1) \\
-\frac{1}{2}\sum^N_{n=1}(1 - t_n)ln|\Sigma|-\frac{1}{2}\sum^N_{n=1}(1-t_n)(x_n-\mu_2)^T\Sigma^{-1}(x_n-\mu_2) \\
= -\frac{N}{2}ln|\Sigma|-\frac{N}{2}Tr\{\Sigma^{-1}S\} \\
\end{align}
\tag{4.77}
$$
where we have defined
$$
S = \frac{N_1}{N}S_1 + \frac{N_2}{N}S_2
\tag{4.78}
$$
$$
S_1 = \frac{1}{N_1}\sum_{n \in C_1}(x_n-\mu_1)(x_n-\mu_1)^T
\tag{4.79}
$$
$$
S_2 = \frac{1}{N_2}\sum_{n \in C_2}(x_n-\mu_2)(x_n-\mu_2)^T
\tag{4.80}
$$
Using the standard result for the maximum likelihood solution for a Gaussian distribution, we see that $\Sigma = S$, which represents a weighted average of the covariance matrices associated with each of the two classes separately.

This result is easily extended to the *K* class problem to obtain the corresponding maximum likelihood solutions for the parameters in which each class-conditional density is Gaussian with a shared covariance matrix. Note that the approach of fitting Gaussian distributions to the classes is not robust to outliers, because the maximum likelihood estimation of a Gaussian is not robust.