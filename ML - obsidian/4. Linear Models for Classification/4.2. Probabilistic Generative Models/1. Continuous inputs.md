Let us assume that the class-conditional densities are Gaussian and then explore the resulting form for the posterior probabilities. To start with, we shall assume that all classes share the same [[covariance matrix]]. Thus the density for class $C_k$ is given by
$$
p(x|C_k) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp\{-\frac{1}{2}(x-\mu_k)\}
\tag{4.64}
$$
Consider first the case of two classes. From (4.57) and (4.58), we have
$$
p(C_1|x) = \sigma(W^Tx + w_0)
\tag{4.65}
$$
where we have defined
$$
W = \Sigma^{-1}(\mu_1 - \mu_2)
\tag{4.66}
$$
$$
w_0 = -\frac{1}{2}\mu^T_1\Sigma^{-1}\mu_1 + \frac{1}{2}\mu^T_2\Sigma^{-1}\mu_2 + ln\frac{p(C_1)}{p(C_2)}
\tag{4.67}
$$
We see that the quadratic terms in **x** from the exponents of the Gaussian densities have cancelled (due to the assumption of common covariance matrices) leading to a linear function of **x** in the argument of the logistic sigmoid. This result is illustrated for the case of a two-dimensional input space **x** in [[Figure 4.10.png|Figure 4.10]]. The resulting decision boundaries correspond to surfaces along which the [[posterior probabilities]] *p($C_k$|**x**)* are constant and so will be given by linear functions of **x**, and therefore the decision boundaries are linear in input space. The [[prior probabilities]] *p($C_k$)* enter only through the bias parameter $w_0$ so that changes in the priors have the effect of making parallel shifts of the decision boundary and more generally of the parallel contours of constant posterior probability.

![[Figure 4.10.png]]
[[Figure 4.10.png|Figure 4.10]]

For the general case of K classes we have, from (4.62) and (4.63),
$$
a_k(x) = W^T_kx + w_0
\tag{4.68}
$$
where we have defined
$$
w_k = \Sigma^{-1}\mu_k
\tag{4.69}
$$
$$
w_{k0} = -\frac{1}{2}\mu^T_k\Sigma^{-1}\mu_k + ln(p(C_k))
\tag{4.70}
$$
We see that the $a_k(x)$ are again linear functions of **x** as a consequence of the cancellation of the quadratic terms due to the shared covariances. The resulting decision boundaries, corresponding to the minimum misclassification rate, will occur when two of the posterior probabilities (the two largest) are equal, and so will be defined by linear functions of **x**, and so again we have a generalized linear model.

If we relax the assumption of a shared covariance matrix and allow each classconditional density $p(x|C_k)$ to have its own covariance matrix $\Sigma_k$, then the earlier cancellations will no longer occur, and we will obtain quadratic functions of **x**, giving rise to a [[quadratic discriminant]]. The linear and quadratic decision boundaries are illustrated in [[Figure 4.11.png|Figure 4.11]].

![[Figure 4.11.png]]
[[Figure 4.11.png|Figure 4.11]]