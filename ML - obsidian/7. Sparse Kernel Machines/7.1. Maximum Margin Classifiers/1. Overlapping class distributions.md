# Overlapping class distribution
So far, we have assumed that the training data points are linearly separable in the
feature space $\phi(x)$. The resulting support vector machine will give exact separation
of the training data in the original input space x, although the corresponding decision boundary will be nonlinear. In practice, however, the class-conditional distributions may overlap, in which case exact separation of the training data can lead to poor generalization.

We therefore need a way to modify the support vector machine so as to allow
some of the training points to be misclassified. From (7.19) we see that in the case
of separable classes, we implicitly used an error function that gave infinite error
if a data point was misclassified and zero error if it was classified correctly, and
then optimized the model parameters to maximize the margin. We now modify this
approach so that data points are allowed to be on the ‘wrong side’ of the margin
boundary, but with a penalty that increases with the distance from that boundary. For the subsequent optimization problem, it is convenient to make this penalty a linear function of this distance. To do this, we introduce [[slack variables]], $\xi_n \geq 0$ where $n = 1, . . . , N$, with one slack variable for each training data point (Bennett, 1992; Cortes and Vapnik, 1995). These are defined by $\xi_n = 0$ for data points that are on or inside the correct margin boundary and $\xi_n = |t_n − y(x_n)|$ for other points. Thus a data point that is on the decision boundary $y(x_n) = 0$ will have $\xi_n = 1$, and points with $\xi_n > 1$ will be misclassified. The exact classification constraints (7.5) are then replaced with
$$
t_ny(x_n) \geq 1 - \xi_n, n = 1,...,N
\tag{7.20}
$$
in which the slack variables are constrained to satisfy $\xi_n \geq 0$. Data points for which
$\xi_n = 0$ are correctly classified and are either on the margin or on the correct side
of the margin. Points for which $0 \lt \xi_n \leq 1$ lie inside the margin, but on the correct
side of the decision boundary, and those data points for which $\xi_n > 1$ lie on
the wrong side of the decision boundary and are misclassified, as illustrated in [[Figure 7.3.png|Figure 7.3]]. This is sometimes described as relaxing the hard margin constraint to give a [[soft margin]] and allows some of the training set data points to be misclassified. Note that while slack variables allow for overlapping class distributions, this framework is still sensitive to outliers because the penalty for misclassification increases linearly with $\xi$.

![[Figure 7.3.png]]
[[Figure 7.3.png|Figure 7.3]]

Our goal is now to maximize the margin while softly penalizing points that lie
on the wrong side of the margin boundary. We therefore minimize
$$
C\sum^N_{n=1}\xi_n+\frac{1}{2}||w||^2
\tag{7.21}
$$
where the parameter $C \gt 0$ controls the trade-off between the slack variable penalty and the margin. Because any point that is misclassified has $\xi_n \gt 1$, it follows that $\sum_n \xi_n$ is an upper bound on the number of misclassified points. The parameter *C* is therefore analogous to (the inverse of) a regularization coefficient because it controls the trade-off between minimizing training errors and controlling model complexity. In the limit $C \rightarrow \infty$, we will recover the earlier support vector machine for separable data.

We now wish to minimize (7.21) subject to the constraints (7.20) together with
$\xi_n \geq 0$. The corresponding Lagrangian is given by
$$
L(w,b,a) = \frac{1}{2}||w||^2 + C\sum^N_{n=1}\xi_n-\sum^N_{n=1}a_n\{t_ny(x_n)- 1 +\xi_n\} - \sum^N_{n=1}\mu_n\xi_n
\tag{7.22}
$$
where $\{a_n \geq 0\}$ and $\{\mu_n \geq 0\}$ are Lagrange multipliers.