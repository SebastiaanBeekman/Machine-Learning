Let us suppose that observations are being drawn from some unknown probability density *p(**x**)* in some *D*-dimensional space, which we shall take to be Euclidean, and we wish to estimate the value of *p(**x**)*. From our earlier discussion of locality, let us consider some small region *R* containing **x**. The probability mass associated with this region is given by
$$
P = \int_Rp(x)dx
\tag{2.242}
$$
Now suppose that we have collected a data set comprising *N* observations drawn from *p(**x**)*. Because each data point has a probability *P* of falling within *R*, the total number *K* of points that lie inside *R* will be distributed according to the binomial distribution
$$
Bin(K|N,P) = \frac{N!}{K!(N - K!)}P^K(1 - P)^{1-K}
\tag{2.243}
$$
$$
\mathbb{E}[m] \equiv \sum^N_{m=0}mBin(m|N,\mu) = N\mu
\tag{2.11}
$$
$$
var[m] \equiv \sum^N_{m=0}(m-\mathbb{E}[m])^2Bin(m|N,\mu) = N\mu(1 - \mu)
\tag{2.12}
$$
Using (2.11), we see that the mean fraction of points falling inside the region is $\mathbb{E}[K/N] = P$, and similarly using (2.12) we see that the variance around this mean is $var[K/N] = P(1 − P)/N$. For large *N*, this distribution will be sharply peaked around the mean and so
$$
K \approx NP
\tag{2.244}
$$
If, however, we also assume that the region *R* is sufficiently small that the probability density *p(**x**)* is roughly constant over the region, then we have
$$
p(x) \approx p(x)V
\tag{2.245}
$$
where V is the volume of R. Combining (2.244) and (2.245), we obtain our density estimate in the form
$$
p(x) = \frac{K}{NV}
\tag{2.246}
$$
Note that the validity of (2.246) depends on two contradictory assumptions, namely that the region *R* be sufficiently small that the density is approximately constant over the region and yet sufficiently large (in relation to the value of that density) that the number *K* of points falling inside the region is sufficient for the binomial distribution to be sharply peaked.

We can exploit the result (2.246) in two different ways. Either we can fix *K* and determine the value of *V* from the data, which gives rise to the K-nearest-neighbour technique discussed shortly, or we can fix *V* and determine *K* from the data, giving rise to the kernel approach. It can be shown that both the K-nearest-neighbour density estimator and the kernel density estimator converge to the true probability density in the limit $N \rightarrow \infty$ provided *V* shrinks suitably with *N*, and *K* grows with *N* (Duda and Hart, 1973).

We begin by discussing the kernel method in detail, and to start with we take the region R to be a small hypercube centred on the point x at which we wish to determine the probability density. In order to count the number K of points falling within this region, it is convenient to define the following function
$$
k(u)=\begin{cases}
1, & |u_i| \leq \frac{1}{2}, i=1,...,D\\
0, & otherwise.
\end{cases}
\tag{2.247}
$$
which represents a unit cube centred on the origin. The function *k(**u**)* is an example of a [[kernel function]], and in this context is also called a [[Parzen window]]. From (2.247), the quantity $k((x−x_n)/h)$ will be one if the data point $x_n$ lies inside a cube of side *h* centred on **x**, and zero otherwise. The total number of data points lying inside this cube will therefore be
$$
K = \sum^N_{n=1}k(\frac{x - x_n}{h})
\tag{2.248}
$$
Substituting this expression into (2.246) then gives the following result for the estimated density at **x**
$$
p(x) = \frac{1}{N}\sum^N_{n=1}\frac{1}{h^D}k(\frac{x-x_n}{h})
\tag{2.249}
$$
where we have used $V = h^D$ for the volume of a hypercube of side *h* in *D* dimensions. Using the symmetry of the function *k(**u**)*, we can now re-interpret this equation, not as a single cube centred on **x** but as the sum over *N* cubes centred on the *N* data points $x_n$.

As it stands, the kernel density estimator (2.249) will suffer from one of the same problems that the histogram method suffered from, namely the presence of artificial discontinuities, in this case at the boundaries of the cubes. We can obtain a smoother density model if we choose a smoother kernel function, and a common choice is the Gaussian, which gives rise to the following kernel density model
$$
p(x) = \frac{1}{N}\sum^N_{n=1}\frac{1}{(2\pi h^2)^{1/2}}exp\{-\frac{||x-x_n||^2}{2h^2}\}
\tag{2.250}
$$
where *h* represents the standard deviation of the Gaussian components. Thus our density model is obtained by placing a Gaussian over each data point and then adding up the contributions over the whole data set, and then dividing by *N* so that the density is correctly normalized. In [[Figure 2.25.png|Figure 2.25]], we apply the model (2.250) to the data set used earlier to demonstrate the histogram technique. We see that, as expected, the parameter *h* plays the role of a smoothing parameter, and there is a trade-off between sensitivity to noise at small *h* and over-smoothing at large *h*. Again, the optimization of *h* is a problem in model complexity, analogous to the choice of bin width in histogram density estimation, or the degree of the polynomial used in curve fitting.

![[Figure 2.25.png]]
[[Figure 2.25.png|Figure 2.25]]

We can choose any other kernel function *k(**u**)* in (2.249) subject to the conditions
$$
k(u) \geq 0
\tag{2.251}
$$
$$
\int k(u)du = 1
\tag{2.252}
$$
which ensure that the resulting probability distribution is nonnegative everywhere and integrates to one. The class of density model given by (2.249) is called a kernel density estimator, or [[Parzen estimator]]. It has a great merit that there is no computation involved in the ‘training’ phase because this simply requires storage of the training set. However, this is also one of its great weaknesses because the computational cost of evaluating the density grows linearly with the size of the data set.

