# Nearest-neighbour methods
One of the difficulties with the kernel approach to density estimation is that the parameter *h* governing the kernel width is fixed for all kernels. In regions of high data density, a large value of *h* may lead to over-smoothing and a washing out of structure that might otherwise be extracted from the data. However, reducing *h* may lead to noisy estimates elsewhere in data space where the density is smaller. Thus the optimal choice for *h* may be dependent on location within the data space. This issue is addressed by nearest-neighbour methods for density estimation.

We therefore return to our general result (2.246) for local density estimation,
and instead of fixing *V* and determining the value of *K* from the data, we consider a fixed value of *K* and use the data to find an appropriate value for 
*V*. To do this, we consider a small sphere centred on the point **x** at which we wish to estimate the density *p(**x**)*, and we allow the radius of the sphere to grow until it contains precisely *K* data points. The estimate of the density *p(**x**)* is then given by (2.246) with *V* set to the volume of the resulting sphere. This technique is known as [[K nearest neighbours]] and is illustrated in [[Figure 2.26.png|Figure 2.26]], for various choices of the parameter *K*, using the same data set as used in [[Figure 2.24.png|Figure 2.24]] and Figure [[Figure 2.25.png|Figure 2.25]]. We see that the value of *K* now governs the degree of smoothing and that again there is an optimum choice for *K* that is neither too large nor too small. Note that the model produced by *K* nearest neighbours is not a true density model because the integral over all space diverges.

![[Figure 2.26.png]]
[[Figure 2.26.png|Figure 2.26]]

We close this chapter by showing how the *K*-nearest-neighbour technique for
density estimation can be extended to the problem of classification. To do this, we apply the *K*-nearest-neighbour density estimation technique to each class separately and then make use of Bayes’ theorem. Let us suppose that we have a data set comprising $N_k$ points in class $C_k$ with *N* points in total, so that $\sum_k N_k = N$. If we wish to classify a new point **x**, we draw a sphere centred on **x** containing precisely *K* points irrespective of their class. Suppose this sphere has volume *V* and contains $K_k$ points from class $C_k$. Then (2.246) provides an estimate of the density associated with each class
$$
p(x|C_k) = \frac{K_k}{N_kV}
\tag{2.253}
$$
Similarly, the unconditional density is given by
$$
p(x) = \frac{K}{NV}
\tag{2.254}
$$
while the class priors are given by
$$
p(C_k) = \frac{N_k}{N}
\tag{2.255}
$$
We can now combine (2.253), (2.254), and (2.255) using Bayes’ theorem to obtain the posterior probability of class membership
$$
p(C_k|x) = \frac{p(x|C_k)p(C_k)}{p(x)} = \frac{K_k}{K}
\tag{2.256}
$$
If we wish to minimize the probability of misclassification, this is done by assigning the test point **x** to the class having the largest [[posterior probability]], corresponding to the largest value of $K_k/K$. Thus to classify a new point, we identify the *K* nearest points from the training data set and then assign the new point to the class having the largest number of representatives amongst this set. Ties can be broken at random. The particular case of $K = 1$ is called the [[nearest-neighbour]] rule, because a test point is simply assigned to the same class as the nearest point from the training set. These concepts are illustrated in [[Figure 2.27.png|Figure 2.27]].

![[Figure 2.27.png]]
[[Figure 2.27.png|Figure 2.27]]

In [[Figure 2.28.png|Figure 2.28]], we show the results of applying the *K*-nearest-neighbour algorithm to the oil flow data, introduced in [[Introduction|Chapter 1]], for various values of 
*K*. As expected, we see that *K* controls the degree of smoothing, so that small *K* produces many small regions of each class, whereas large *K* leads to fewer larger regions.

![[Figure 2.28.png]]
[[Figure 2.28.png|Figure 2.28]]

An interesting property of the [[nearest-neighbour]] classifier is that, in the limit $N \rightarrow \infty$, the error rate is never more than twice the minimum achievable error rate of an optimal classifier, i.e., one that uses the true class distributions (Cover and Hart, 1967).

As discussed so far, both the *K*-nearest-neighbour method, and the kernel density estimator, require the entire training data set to be stored, leading to expensive computation if the data set is large. This effect can be offset, at the expense of some additional one-off computation, by constructing tree-based search structures to allo (approximate) near neighbours to be found efficiently without doing an exhaustive search of the data set. Nevertheless, these nonparametric methods are still severely limited. On the other hand, we have seen that simple parametric models are very restricted in terms of the forms of distribution that they can represent. We therefore need to find density models that are very flexible and yet for which the complexity of the models can be controlled independently of the size of the training set, and we shall see in subsequent chapters how to achieve this.