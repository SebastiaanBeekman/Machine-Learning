# Probability Distributions
In [[Introduction|Chapter 1]], we emphasized the central role played by probability theory in the solution of pattern recognition problems. We turn now to an exploration of some particular examples of probability distributions and their properties. As well as being of great interest in their own right, these distributions can form building blocks for more complex models and will be used extensively throughout the book. The distributions introduced in this chapter will also serve another important purpose, namely to provide us with the opportunity to discuss some key statistical concepts, such as Bayesian inference, in the context of simple models before we encounter them in more complex situations in later chapters.

One role for the distributions discussed in this chapter is to model the probability distribution *p(x)* of a random variable *x*, given a finite set $x_1, ..., x_N$ of observations. This problem is known as [[density estimation]]. For the purposes of this chapter, we shall assume that the data points are independent and identically distributed. It should be emphasized that the problem of density estimation is fundamentally ill-posed, because there are infinitely many probability distributions that could have given rise to the observed finite data set. Indeed, any distribution *p(x)* that is nonzero at each of the data points $x_1, ..., x_N$ is a potential candidate. The issue of choosing an appropriate distribution relates to the problem of model selection that has already been encountered in the context of polynomial curve fitting in [[Introduction|Chapter 1]] and that is a central issue in pattern recognition.

We begin by considering the binomial and multinomial distributions for discrete random variables and the Gaussian distribution for continuous random variables. These are specific examples of [[parametric]] distributions, so-called because they are governed by a small number of adaptive parameters, such as the mean and variance in the case of a Gaussian for example. To apply such models to the problem of density estimation, we need a procedure for determining suitable values for the parameters, given an observed data set. In a frequentist treatment, we choose specific values for the parameters by optimizing some criterion, such as the likelihood function. By contrast, in a Bayesian treatment we introduce prior distributions over the parameters and then use Bayesâ€™ theorem to compute the corresponding posterior distribution given the observed data.

We shall see that an important role is played by [[conjugate]] priors, that lead to
posterior distributions having the same functional form as the prior, and that therefore lead to a greatly simplified Bayesian analysis. For example, the conjugate prior for the parameters of the multinomial distribution is called the [[Dirichlet]] distribution, while the conjugate prior for the mean of a Gaussian is another Gaussian. All of these distributions are examples of the [[exponential family]] of distributions, which possess a number of important properties, and which will be discussed in some detail. 

One limitation of the parametric approach is that it assumes a specific functional form for the distribution, which may turn out to be inappropriate for a particular application. An alternative approach is given by [[nonparametric]] density estimation methods in which the form of the distribution typically depends on the size of the data set. Such models still contain parameters, but these control the model complexity rather than the form of the distribution. We end this chapter by considering three nonparametric methods based respectively on histograms, nearest-neighbours, and kernels.

## Index
- [[Binary Variables]]
- [[Multinomial Variables]]
- [[The Gaussian Distribution]]
- [[The Exponential Familiy]]
- [[Nonparametric Methods]]