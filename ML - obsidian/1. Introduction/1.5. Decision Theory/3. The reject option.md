We have seen that classification errors arise from the regions of input space where the largest of the [[posterior probabilities]] *p($C_k$|**x**)* is significantly less than unity, or equivalently where the joint distributions *p(**x**, $C_k$)* have comparable values. These are the regions where we are relatively uncertain about class membership. In some applications, it will be appropriate to avoid making decisions on the difficult cases in anticipation of a lower error rate on those examples for which a classification decision is made. This is known as the [[reject option]]. For example, in our hypothetical medical illustration, it may be appropriate to use an automatic system to classify those X-ray images for which there is little doubt as to the correct class, while leaving a human expert to classify the more ambiguous cases. We can achieve this by introducing a threshold $\theta$ and rejecting those inputs **x** for which the largest of the posterior probabilities *p($C_k$|**x**)* is less than or equal to $\theta$. This is illustrated for the case of two classes, and a single continuous input variable **x**, in [[Figure 1.26.png|Figure 1.26]]. Note that setting $\theta$ = 1 will ensure that all examples are rejected, whereas if there are *K* classes then setting $\theta$ < 1/*K* will ensure that no examples are rejected. Thus the fraction of examples that get rejected is controlled by the value of $\theta$.

![[Figure 1.26.png]]
[[Figure 1.26.png|Figure 1.26]]

We can easily extend the reject criterion to minimize the expected loss, when a loss matrix is given, taking account of the loss incurred when a reject decision is made.