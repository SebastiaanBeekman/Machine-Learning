# Interference and decision
We have broken the classification problem down into two separate stages, the [[inference stage]] in which we use training data to learn a model for *p($C_k$|**x**)*, and the subsequent [[decision stage]] in which we use these posterior probabilities to make optimal class assignments. An alternative possibility would be to solve both problems together and simply learn a function that maps inputs **x** directly into decisions. Such a function is called a [[discriminant function]].

In fact, we can identify three distinct approaches to solving decision problems,
all of which have been used in practical applications. These are given, in decreasing order of comple$x_I$ty, by:

**(a)**
	First solve the inference problem of determining the class-conditional densities *p(**x**|$C_k$)* for each class $C_k$ individually. Also separately infer the prior class probabilities *p($C_k$)*. Then use Bayes’ theorem in the form
$$
p(C_k|x) = \frac{p(x|C_k)p(C_k)}{p(x)}
\tag{1.82}
$$
	to find the posterior class probabilities *p($C_k$|**x**)*. As usual, the denominator in Bayes’ theorem can be found in terms of the quantities appearing in the numerator, because
$$
p(x) = \sum_kp(x|C_k)p(C_k)
\tag{1.83}
$$
	Equivalently, we can model the joint distribution *p(**x**, $C_k$)* directly and then normalize to obtain the posterior probabilities. Having found the posterior probabilities, we use decision theory to determine class membership for each new input **x**. Approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as [[generative models]], because by sampling	from them it is possible to generate synthetic data points in the input space.

**(b)**
	First solve the inference problem of determining the posterior class probabilities *p($C_k$|**x**)*, and then subsequently use decision theory to assign each new **x** to one of the classes. Approaches that model the posterior probabilities directly are called [[discriminative models]].

**(c)**
	Find a function *f(**x**)*, called a discriminant function, which maps each input **x** directly onto a class label. For instance, in the case of two-class problems, f(·) might be binary valued and such that *f* = 0 represents class $C_1$ and *f* = 1 represents class $C_2$. In this case, probabilities play no role.

Let us consider the relative merits of these three alternatives. Approach (a) is the most demanding because it involves finding the joint distribution over both **x** and $C_k$. For many applications, **x** will have high dimensionality, and consequently we may need a large training set in order to be able to determine the class conditional densities to reasonable accuracy. Note that the class priors *p($C_k$)* can often be estimated simply from the fractions of the training set data points in each of the classes. One advantage of approach (a), however, is that it also allows the marginal density of data *p(**x**)* to be determined from (1.83). This can be useful for detecting new data points that have low probability under the model and for which the predictions may be of low accuracy, which is known as [[outlier detection]] or [[novelty detection]] (Bishop, 1994; Tarassenko, 1995).

However, if we only wish to make classification decisions, then it can be wasteful
of computational resources, and excessively demanding of data, to find the joint
distribution *p(**x**, $C_k$)* when in fact we only really need the [[posterior probabilities]]
*p($C_k$|**x**)*, which can be obtained directly through approach (b). Indeed, the classconditional densities may contain a lot of structure that has little effect on the posterior probabilities, as illustrated in [[Figure 1.27.png|Figure 1.27]]. There has been much interest in exploring the relative merits of generative and discriminative approaches to machine learning, and in finding ways to combine them (Jebara, 2004; Lasserre et al., 2006).

![[Figure 1.27.png]]
[[Figure 1.27.png|Figure 1.27]]

An even simpler approach is (c) in which we use the training data to find a
discriminant function *f(**x**)* that maps each **x** directly onto a class label, thereby
combining the inference and decision stages into a single learning problem. In the example of [[Figure 1.27.png|Figure 1.27]], this would correspond to finding the value of **x** shown by the vertical green line, because this is the decision boundary giving the minimum probability of misclassification.

With option (c), however, we no longer have access to the [[posterior probabilities]]
*p($C_k$|**x**)*. There are many powerful reasons for wanting to compute the posterior
probabilities, even if we subsequently use them to make decisions. These include:

**Minimizing risk.**
	Consider a problem in which the elements of the loss matrix are subjected to revision from time to time (such as might occur in a financial application). If we know the posterior probabilities, we can trivially revise the minimum risk decision criterion by modifying (1.81) appropriately. If we have only a discriminant function, then any change to the loss matrix would require that we return to the training data and solve the classification problem afresh.

**Reject option.**
	Posterior probabilities allow us to determine a rejection criterion that will minimize the misclassification rate, or more generally the expected loss, for a given fraction of rejected data points.

**Compensating for class priors.**
	Consider our medical X-ray problem again, and suppose that we have collected a large number of X-ray images from the general population for use as training data in order to build an automated screening system. Because cancer is rare amongst the general population, we might find that, say, only 1 in every 1,000 examples corresponds to the presence of cancer. If we used such a data set to train an adaptive model, we could run into severe difficulties due to the small proportion of the cancer class. For instance, a classifier that assigned every point to the normal class would already achieve 99.9% accuracy and it would be difficult to avoid this trivial solution. Also, even a large data set will contain very few examples of X-ray images corresponding to cancer, and so the learning algorithm will not be exposed to a broad range of examples of such images and hence is not likely to generalize well. A balanced data set in which we have selected equal numbers of examples from each of the classes would allow us to find a more accurate model. However, we then have to compensate for the effects of our modifications to the training data. Suppose we have used such a modified data set and found models for the posterior probabilities. From Bayes’ theorem (1.82), we see that the posterior probabilities are proportional to the prior probabilities, which we can interpret as the fractions of points in each class. We can therefore simply take the posterior probabilities obtained from our artificially balanced data set and first divide by the class fractions in that data set and then multiply by the class fractions in the population to which we wish to apply the model. Finally, we need to normalize to ensure that the new posterior probabilities sum to one. Note that this procedure cannot be applied if we have learned a discriminant function directly instead of determining posterior probabilities.

**Combining models.**
	For complex applications, we may wish to break the problem into a number of smaller subproblems each of which can be ta$C_k$led by a separate module. For example, in our hypothetical medical diagnosis problem, we may have information available from, say, blood tests as well as X-ray images. Rather than combine all of this heterogeneous information into one huge input space, it may be more effective to build one system to interpret the Xray images and a different one to interpret the blood data. As long as each of the two models gives posterior probabilities for the classes, we can combine the outputs systematically using the rules of probability. One simple way to do this is to assume that, for each class separately, the distributions of inputs for the X-ray images, denoted by $x_I$, and the blood data, denoted by $x_B$, are independent, so that
$$
p(x_I, x_B|C_k) = p(x_I|C_k)p(x_B|C_k)
\tag{1.84}
$$
	This is an example of [[conditional independence]] property, because the independence holds when the distribution is conditioned on the class $C_k$. The posterior probability, given both the X-ray and blood data, is then given by
$$
\begin{align}
p(C_k|x_I, x_B) \propto p(x_I, x_B|C_k)p(C_k) \\
\propto p(x_I|C_k)p(x_B|C_k)p(C_k)\\
\propto \frac{p(C_k|x_I)p(C_k|x_B)}{p(C_k)}
\end{align}
\tag{1.85}
$$
	Thus we need the class prior probabilities *p($C_k$)*, which we can easily estimate from the fractions of data points in each class, and then we need to normalize the resulting posterior probabilities so they sum to one. The particular conditional independence assumption (1.84) is an example of the [[naive Bayes model]]. Note that the joint marginal distribution p($x_I$, $x_B$) will typically not factorize under this model. We shall see in later chapters how to construct models for combining data that do not require the conditional independence assumption (1.84).
	