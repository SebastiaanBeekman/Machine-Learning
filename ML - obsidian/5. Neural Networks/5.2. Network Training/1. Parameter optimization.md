# Parameter optimization
We turn next to the task of finding a weight vector **w** which minimizes the
chosen function $E(w)$. At this point, it is useful to have a geometrical picture of the
error function, which we can view as a surface sitting over weight space as shown in
[[Figure 5.5.png|Figure 5.5]]. First note that if we make a small step in weight space from *w* to $w+\delta w$
then the change in the error function is $\delta E \approx \delta w^T\nabla E(w)$, where the vector $\nabla E(w)$
points in the direction of greatest rate of increase of the error function. Because the
error $E(w)$ is a smooth continuous function of **w**, its smallest value will occur at a point in weight space such that the gradient of the error function vanishes, so that
$$
\nabla E(w) = 0
\tag{5.26}
$$
as otherwise we could make a small step in the direction of $âˆ’\nabla E(w)$ and thereby
further reduce the error. Points at which the gradient vanishes are called stationary
points, and may be further classified into minima, maxima, and saddle points.

![[Figure 5.5.png]]
[[Figure 5.5.png|Figure 5.5]]

Our goal is to find a vector w such that $E(w)$ takes its smallest value. However,
the error function typically has a highly nonlinear dependence on the weights
and bias parameters, and so there will be many points in weight space at which the
gradient vanishes (or is numerically very small). Indeed, from the discussion in [[1. Weight-space symmetries|Section 5.1.1]] we see that for any point **w** that is a local minimum, there will be other
points in weight space that are equivalent minima. For instance, in a two-layer network of the kind shown in [[Figure 5.1.png|Figure 5.1]], with *M* hidden units, each point in weight space is a member of a family of $M!2^M$ equivalent points.

Furthermore, there will typically be multiple inequivalent stationary points and
in particular multiple inequivalent minima. A minimum that corresponds to the
smallest value of the error function for any weight vector is said to be a [[global minimum]]. Any other minima corresponding to higher values of the error function
are said to be [[local minima]]. For a successful application of neural networks, it may
not be necessary to find the global minimum (and in general it will not be known
whether the global minimum has been found) but it may be necessary to compare
several local minima in order to find a sufficiently good solution.

Because there is clearly no hope of finding an analytical solution to the equation
$\nabla E(w) = 0$ we resort to iterative numerical procedures. The optimization of
continuous nonlinear functions is a widely studied problem and there exists an extensive literature on how to solve it efficiently. Most techniques involve choosing
some initial value $w^{(0)}$ for the weight vector and then moving through weight space
in a succession of steps of the form
$$
w^{(\tau+1)} = w^{(\tau)} + \nabla w^{(\tau)}
\tag{5.27}
$$
where $\tau$ labels the iteration step. Different algorithms involve different choices for
the weight vector update $\Delta w(\tau)$. Many algorithms make use of gradient information and therefore require that, after each update, the value of $\nabla E(w)$ is evaluated at the new weight vector $w(\tau +1)$. In order to understand the importance of gradient information, it is useful to consider a local approximation to the error function based on a Taylor expansion.