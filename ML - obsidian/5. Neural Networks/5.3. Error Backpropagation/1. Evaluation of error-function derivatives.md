We now derive the backpropagation algorithm for a general network having arbitrary feed-forward topology, arbitrary differentiable nonlinear activation functions, and a broad class of error function. The resulting formulae will then be illustrated using a simple layered network structure having a single layer of sigmoidal hidden units together with a sum-of-squares error.

Many error functions of practical interest, for instance those defined by maximum likelihood for a set of i.i.d. data, comprise a sum of terms, one for each data point in the training set, so that
$$
E(w) = \sum^N_{n=1}E_n(w)
\tag{5.44}
$$
Here we shall consider the problem of evaluating $\nabla E_n(w)$ for one such term in the error function. This may be used directly for sequential optimization, or the results can be accumulated over the training set in the case of batch methods.

Consider first a simple linear model in which the outputs $y_k$ are linear combinations of the input variables $x_i$ so that
$$
y_k = \sum_{i}w_{ki}x_i
\tag{5.45}
$$
together with an error function that, for a particular input pattern *n*, takes the form
$$
E_n = \frac{1}{2}\sum_{k}(y_{nk} - t_{nk})^2
\tag{5.46}
$$
where $y_{nk} = y_k(x_n,w)$. The gradient of this error function with respect to a weight $w_{ji}$ is given by
$$
\frac{\partial E_n}{\partial w_{Ji}} = (y_{nj} - t_{nj})x_{ni}
\tag{5.47}
$$
which can be interpreted as a ‘local’ computation involving the product of an ‘error signal’ $y_{nj} − t_{nj}$ associated with the output end of the link $w_{ji}$ and the variable $x_{ni}$ associated with the input end of the link. In [[2. Logistic regression|Section 4.3.2]], we saw how a similar formula arises with the logistic sigmoid activation function together with the cross entropy error function, and similarly for the softmax activation function together with its matching cross-entropy error function. We shall now see how this simple
result extends to the more complex setting of multilayer feed-forward networks.

In a general feed-forward network, each unit computes a weighted sum of its inputs of the form
$$
a_j = \sum_{i}w_{ji}z_i
\tag{5.48}
$$
where $z_i$ is the activation of a unit, or input, that sends a connection to unit j, and $w_{ji}$ is the weight associated with that connection. In [[Feed-forward Network Functions|Section 5.1]], we saw that biases can be included in this sum by introducing an extra unit, or input, with activation fixed at +1. We therefore do not need to deal with biases explicitly. The sum in (5.48) is transformed by a nonlinear activation function $h(·)$ to give the activation $z_j$ of unit *j* in the form
$$
z_j = h(a_j)
\tag{5.49}
$$
Note that one or more of the variables $z_i$ in the sum in (5.48) could be an input, and similarly, the unit *j* in (5.49) could be an output.

For each pattern in the training set, we shall suppose that we have supplied the corresponding input vector to the network and calculated the activations of all of the hidden and output units in the network by successive application of (5.48) and (5.49). This process is often called [[forward propagation]] because it can be regarded as a forward flow of information through the network.

Now consider the evaluation of the derivative of $E_n$ with respect to a weight $w_{ji}$. The outputs of the various units will depend on the particular input pattern n. However, in order to keep the notation uncluttered, we shall omit the subscript n from the network variables. First we note that $E_n$ depends on the weight $w_{ji}$ only via the summed input aj to unit j. We can therefore apply the chain rule for partial derivatives to give
$$
\frac{\partial E_n}{\partial w_{ji}} = \frac{\partial E_n}{\partial a_j}\frac{\partial a_j}{\partial w_{ji}}
\tag{5.50}
$$
We now introduce a useful notation
$$
\delta_j = \frac{\partial E_n}{\partial a_{j}}
\tag{5.51}
$$
where the $\delta$’s are often referred to as errors for reasons we shall see shortly. Using (5.48), we can write
$$
\frac{\partial a_j}{\partial w_{ij}} = z_i
\tag{5.52}
$$
Substituting (5.51) and (5.52) into (5.50), we then obtain
$$
\frac{\partial E_n}{\partial w_{ij}} = \delta_jz_i
\tag{5.54}
$$
Equation (5.53) tells us that the required derivative is obtained simply by multiplying the value of $\delta$ for the unit at the output end of the weight by the value of *z* for the unit at the input end of the weight (where *z* = 1in the case of a bias). Note that this takes the same form as for the simple linear model considered at the start of this section. Thus, in order to evaluate the derivatives, we need only to calculate the value of $\delta_j$ for each hidden and output unit in the network, and then apply (5.53).

As we have seen already, for the output units, we have
$$
\delta_k = y_k - t_k
\tag{5.54}
$$
provided we are using the canonical link as the output-unit activation function. To evaluate the $\delta$’s for hidden units, we again make use of the chain rule for partial derivatives,
$$
\delta_j = \frac{\partial E_n}{\partial a_j} = \sum_{k}\frac{\partial E_n}{\partial a_k}\frac{\partial a_k}{\partial a_j}
\tag{5.55}
$$
where the sum runs over all units k to which unit j sends connections. The arrangement of units and weights is illustrated in [[Figure 5.7.png|Figure 5.7]]. Note that the units labelled *k* could include other hidden units and/or output units. In writing down (5.55), we are making use of the fact that variations in $a_j$ give rise to variations in the error function only through variations in the variables $a_k$. If we now substitute the definition of $\delta$ given by (5.51) into (5.55), and make use of (5.48) and (5.49), we obtain the following [[backpropagation formula]]
$$
\delta_j = h'(a_j)\sum_{k}w_{kj}\delta_k
\tag{5.56}
$$
which tells us that the value of $\delta$ for a particular hidden unit can be obtained by propagating the $\delta$’s backwards from units higher up in the network, as illustrated in [[Figure 5.7.png|Figure 5.7]]. Note that the summation in (5.56) is taken over the first index on $w_{kj}$ (corresponding to backward propagation of information through the network), whereas in the forward propagation equation (5.10) it is taken over the second index. Because we already know the values of the $\delta$’s for the output units, it follows that by recursively applying (5.56) we can evaluate the $\delta$’s for all of the hidden units in a feed-forward network, regardless of its topology.

![[Figure 5.7.png]]
[[Figure 5.7.png|Figure 5.7]]

The backpropagation procedure can therefore be summarized as follows.
1. Apply an input vector $x_n$ to the network and forward propagate through the network using (5.48) and (5.49) to find the activations of all the hidden and output units
2. Evaluate the $\delta_k$ for all the output units using (5.54)
3. Backpropagate the $\delta$’s using (5.56) to obtain $\delta_j$ for each hidden unit in the network
4. Use (5.53) to evaluate the required derivatives

For batch methods, the derivative of the total error *E* can then be obtained by repeating the above steps for each pattern in the training set and then summing over all patterns:
$$
\frac{\partial E}{\partial w_{ji}} = \sum_{n}\frac{\partial E_n}{\partial w_{ji}}
\tag{5.57}
$$
In the above derivation we have implicitly assumed that each hidden or output unit in the network has the same activation function $h(·)$. The derivation is easily generalized, however, to allow different units to have individual activation functions, simply by keeping track of which form of $h(·)$ goes with which unit.