# Geometry of least squares
At this point, it is instructive to consider the geometrical interpretation of the
least-squares solution. To do this we consider an *N*-dimensional space whose axes
are given by the tn, so that $t = (t_1, . . . , t_N)^T$ is a vector in this space. Each basis
function $\phi_j(x_n)$, evaluated at the *N* data points, can also be represented as a vector in the same space, denoted by $\phi_j$ , as illustrated in [[Figure 3.2.png|Figure 3.2]]. Note that $\phi_j$ corresponds to the $j^{th}$ column of $\Phi$, whereas $\phi(x_n)$ corresponds to the $n^{th}$ row of $\Phi$. If the number *M* of basis functions is smaller than the number *N* of data points, then the *M* vectors $\phi_j(x_n)$ will span a linear subspace *S* of dimensionality *M*. We define **y** to be an *N*-dimensional vector whose $n^{th}$ element is given by $y(x_n,w)$, where $n = 1, . . . , N$. Because **y** is an arbitrary linear combination of the vectors $\phi_j$ , it can live anywhere in the *M*-dimensional subspace. The sum-of-squares error (3.12) is then equal (up to a factor of 1/2) to the squared Euclidean distance between **y** and **t**. Thus the least-squares solution for **w** corresponds to that choice of **y** that lies in subspace *S* and that is closest to **t**. Intuitively, from [[Figure 3.2.png|Figure 3.2]], we anticipate that this solution corresponds to the orthogonal projection of **t** onto the subspace *S*. This is indeed the case, as can easily be verified by noting that the solution for **y** is given by $\Phi w_{ML}$, and then confirming that this takes the form of an orthogonal projection.

![[Figure 3.2.png]]
[[Figure 3.2.png|Figure 3.2]]

In practice, a direct solution of the normal equations can lead to numerical difficulties when $\Phi^T\Phi$ is close to singular. In particular, when two or more of the basis vectors $\phi_j$ are co-linear, or nearly so, the resulting parameter values can have large magnitudes. Such near degeneracies will not be uncommon when dealing with real data sets. The resulting numerical difficulties can be addressed using the technique of [[singular value decomposition]], or SVD (Press et al., 1992; Bishop and Nabney, 2008). Note that the addition of a regularization term ensures that the matrix is nonsingular, even in the presence of degeneracies.