In [[Introduction|Chapter 1]], we fitted polynomial functions to data sets by minimizing a sum-of-squares error function. We also showed that this error function could be motivated as the maximum likelihood solution under an assumed Gaussian noise model. Let us return to this discussion and consider the least squares approach, and its relation to maximum likelihood, in more detail.

As before, we assume that the target variable *t* is given by a deterministic function $y(x,w)$ with additive Gaussian noise so that
$$
t = y(x,w) + \epsilon
\tag{3.7}
$$
where $\epsilon$ is a zero mean Gaussian random variable with precision (inverse variance) $\beta$. Thus we can write
$$
p(t|x,w,\beta) = N(t|y(x,w),\beta^{-1})
\tag{3.8}
$$
Recall that, if we assume a squared loss function, then the optimal prediction, for a new value of x, will be given by the conditional mean of the target variable. In the case of a Gaussian conditional distribution of the form (3.8), the conditional mean will be simply
$$
\mathbb{E}[t|x] = \int tp(t|x)dt = y(x,w)
\tag{3.9}
$$
Note that the Gaussian noise assumption implies that the conditional distribution of *t* given *x* is unimodal, which may be inappropriate for some applications. An extension to mixtures of conditional Gaussian distributions, which permit multimodal conditional distributions, will be discussed in [[Section 14.5.1]].

Now consider a data set of inputs $X = \{x_1, ..., x_N\}$ with corresponding target values $t_1, ..., t_N$. We group the target variables $\{t_n\}$ into a column vector that we denote by **t** where the typeface is chosen to distinguish it from a single observation of a multivariate target, which would be denoted **t**. Making the assumption that these data points are drawn independently from the distribution (3.8), we obtain the following expression for the likelihood function, which is a function of the adjustable parameters w and $\beta$, in the form
$$
p(t|X,w,\beta) = \prod^N_{n=1}N(t_n|w^T\phi(x_n),\beta^{-1})
\tag{3.10}
$$
where we have used (3.3). Note that in supervised learning problems such as regression (and classification), we are not seeking to model the distribution of the input variables. Thus x will always appear in the set of conditioning variables, and so from now on we will drop the explicit x from expressions such as $p(t|x,w, \beta)$ in order to keep the notation uncluttered. Taking the logarithm of the likelihood function, and making use of the standard form (1.46) for the univariate Gaussian, we have
$$
\begin{align}
ln(p(t|w,\beta)) = \sum^N_{n=1}ln(N(t_n|w^T\phi(x_n),\beta^{-1})) \\
= \frac{N}{2}ln(\beta-\frac{N}{2})ln(2\pi)-\beta E_D(w)
\end{align}
\tag{3.11}
$$
where the sum-of-squares error fucntion is defined by
$$
E_d(w) = \frac{1}{2}\sum^N_{n=1}\{t_n-w^T\phi(x_n)\}^2
\tag{3.12}
$$
Having written down the likelihood function, we can use maximum likelihood to determine w and $\beta$. Consider first the maximization with respect to w. As observed already in [[Section 1.2.5]], we see that maximization of the likelihood function under a conditional Gaussian noise distribution for a linear model is equivalent to minimizing a sum-of-squares error function given by $E_D(w)$. The gradient of the log likelihood function (3.11) takes the form
$$
\nabla ln(p(t|w,\beta)) = \sum^N_{n=1}\{t_n-w^T\phi(x_n)\}\phi(x_n)^T
\tag{3.13}
$$
Setting this gradient to zero gives
$$
0 = \sum^N_{n=1}t_n\phi(x_n)^T-w^T(\sum^N_{n=1}\phi(x_n)\phi(x_n)^T)
\tag{3.14}
$$
Solving for w we obtain
$$
w_{ML}=(\Phi^T\Phi)^{-1}\Phi^Tt
\tag{3.15}
$$
which are known as the normal equations for the least squares problem. Here $\Phi$ is an $N×M$ matrix, called the [[design matrix]], whose elements are given by $\Phi_{nj} = \Phi_j(x_n)$, so that
$$
\Phi = 
\begin{pmatrix}
\phi_0(x_1) & \phi_1(x_1) & \cdots & \phi_{M-1}(x_1) \\
\phi_0(x_1) & \phi_1(x_1) & \cdots & \phi_{M-1}(x_1) \\
\vdots & \vdots & \ddots & \vdots \\
\phi_0(x_N) & \phi_1(x_N) & \cdots & \phi_{M-1}(x_N) \\
\end{pmatrix}
\tag{3.16}
$$
The quantity
$$
\Phi^\dagger \equiv (\Phi^T\Phi)^{-1}\Phi^T
\tag{3.17}
$$
is known as the [[Moore-Penrose pseudo-inverse]] of the matrix $\Phi$ (Rao and Mitra, 1971; Golub and Van Loan, 1996). It can be regarded as a generalization of the notion of matrix inverse to nonsquare matrices. Indeed, if $\Phi$ is square and invertible, then using the property $(AB)^{−1} = B^{−1}A^{−1}$ we see that $\Phi^\dagger\equiv \Phi^{-1}$.

At this point, we can gain some insight into the role of the bias parameter $w_0$. If we make the bias parameter explicit, then the error function (3.12) becomes
$$
E_D(w)=\frac{1}{2}\sum^N_{n=1}\{t_n-w_0-\sum^{m-1}_{j=1}w_j\phi_j(x_n))\}^2
\tag{3.18}
$$
Setting the derivative with respect to $w_0$ equal to zero, and solving for $w_0$, we obtain
$$
w_0 = \overline{t} - \sum^{m-1}_{j=1}w_j\overline{\phi_j}
\tag{3.19}
$$
where we have defined
$$
\begin{align}
\overline{t}\frac{1}{N}\sum^N_{n=1}t_n \\
\overline{\phi_j} = \frac{1}{N}\sum^N_{n=1}\phi_j(x_n)
\end{align}
\tag{3.20}
$$
Thus the bias $w_0$ compensates for the difference between the averages (over the training set) of the target values and the weighted sum of the averages of the basis function values.

We can also maximize the log likelihood function (3.11) with respect to the noise precision parameter $\beta$, giving
$$
\frac{1}{\beta_{ML}} = \frac{1}{N}\sum^N_{n=1}\{t_n-w^T_{ML}\phi(x_n)\}^2
\tag{3.21}
$$
and so we see that the inverse of the noise precision is given by the residual variance of the target values around the regression function.
