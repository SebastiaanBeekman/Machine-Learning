Batch techniques, such as the maximum likelihood solution (3.15), which involve processing the entire training set in one go, can be computationally costly for large data sets. As we have discussed in [[Introduction|Chapter 1]], if the data set is sufficiently large, it may be worthwhile to use [[sequential algorithms]], also known as [[on-line]] algorithms, in which the data points are considered one at a time, and the model parameters updated after each such presentation. Sequential learning is also appropriate for realtime applications in which the data observations are arriving in a continuous stream, and predictions must be made before all of the data points are seen.

We can obtain a sequential learning algorithm by applying the technique of [[stochastic gradient descent]], also known as [[sequential gradient descent]], as follows. If the error function comprises a sum over data points $E = \sum_nE_n$, then after presentation of pattern *n*, the stochastic gradient descent algorithm updates the parameter vector **w** using
$$
w^{(\tau+1)}=w^{(\tau)} - \eta\nabla E_n
\tag{3.22}
$$
where $\tau$ denotes the iteration number, and $\eta$ is a learning rate parameter. We shall discuss the choice of value for $\eta$ shortly. The value of **w** is initialized to some starting vector $w^{(0)}$. For the case of the sum-of-squares error function (3.12), this gives
$$
w^{(\tau+1)}=w^{(\tau)} + \eta(t_n-w^{(\tau)T}\phi_n)\phi_n
\tag{3.23}
$$
where $\phi_n = \phi(x_n)$. This is known as [[least-mean-squares]] or the [[LMS algorithm]]. The value of $\eta$ needs to be chosen with care to ensure that the algorithm converges (Bishop and Nabney, 2008).